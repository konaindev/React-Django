# Javascript Node CircleCI 2.0 configuration file
#
# Check https://circleci.com/docs/2.0/language-javascript/ for more details
#
version: 2.1

orbs:
  aws-cli: circleci/aws-cli@0.1.1
  node: circleci/node@1.1.3
  heroku: circleci/heroku@0.0.8
  gcp-cli: circleci/gcp-cli@1.8.3

jobs:
  build:
    docker:
      - image: circleci/python:3.7.2-node
        environment:
          DATABASE_URL: postgresql://root@localhost/circle_test
      - image: circleci/redis:latest
      - image: nimbustech/postgres-ssl:9.5
        environment:
          POSTGRES_USER: root
          POSTGRES_DB: circle_test

    working_directory: ~/repo

    steps:
      - checkout

      # Download and cache dependencies
      - restore_cache:
          keys:
            - v1-dependencies-{{ checksum "package.json" }}
            # fallback to using the latest cache if no exact match is found
            - v1-dependencies-

      - run: yarn install --silent
      - run: pipenv --bare install
      - run: yarn build --silent
      - run: pipenv run python manage.py collectstatic
      - save_cache:
          paths:
            - node_modules
          key: v1-dependencies-{{ checksum "package.json" }}

      # run tests!
      - run: yarn test-ui
      - run: yarn storybook-smoke
      - run: pipenv run python manage.py test
      - run: pipenv run python manage.py migrate
      - run: pipenv run python manage.py loaddata data/dumped/latest.json

  # NOTE: This can should likely be added to the PROD deployment
  #       workflow. The idea being that if we have a single source
  #       of truth via storybook/chromatic then we those assets should
  #       be valid before we hit production...
  #
  chromatic-storybook:
    docker:
      - image: circleci/python:3.7.2-node
      - image: circleci/postgres:latest
      - image: circleci/redis:latest

    working_directory: ~/repo

    steps:
      - checkout

      # Download and cache dependencies
      - restore_cache:
          keys:
            - v1-dependencies-{{ checksum "package.json" }}
            # fallback to using the latest cache if no exact match is found
            - v1-dependencies-

      - run: yarn install --silent
      - run: pipenv --bare install
      - run: yarn build --silent
      - run: pipenv run python manage.py collectstatic
      - save_cache:
          paths:
            - node_modules
          key: v1-dependencies-{{ checksum "package.json" }}

      - run: yarn run chromatic

  deploy-airflow-staging:
    docker:
      - image: google/cloud-sdk
    
    steps:
      - run:
          name: Install Heroku Tools
          command: curl https://cli-assets.heroku.com/install.sh | sh
      - checkout
      - gcp-cli/initialize
      - gcp-cli/install
      - run:
          name: Set COMPOSER environment
          command:  |
            echo "export COMPOSER_ENV=remarkably-development" >> $BASH_ENV
      - run:
          name: Set COMPOSER_AIRFLOW_ENV environment
          command:  |
            echo "export COMPOSER_AIRFLOW_ENV=true" >> $BASH_ENV
      - run:
          name: Pause running dags
          command:    |
              python3 -c "import remark_airflow.setup_utils as setup_utils; setup_utils.pause_all_dags()" || echo "broken dag somewhere"
#      - run:
#          name: Compare and Update PyPi packages
#          command: python3 -c "import remark_airflow.setup_utils as setup_utils; setup_utils.check_pypi_packages()"
#          no_output_timeout: 30m
      - run: 
          name: Set Composer BUCKET
          command:    |
              get_bucket=$(gcloud composer environments describe $COMPOSER_ENV \
              --location us-central1 \
              --format="get(config.dagGcsPrefix)" | sed -e 's/gs:\/\///g' -e's/\/dags//g')
              echo "export BUCKET=$get_bucket" >> $BASH_ENV
      - run:
          name: Get DATABASE_URL
          command: echo "export DATABASE_URL=$(heroku config:get DATABASE_URL -a remarkably-staging)" >> $BASH_ENV
      - run:
          name: Add environment variables to Composer Environment
          command: python3 -c "import remark_airflow.setup_utils as setup_utils; setup_utils.export_environment_variables()"
          no_output_timeout: 30m
      # NOTE: gsutil rsync sync will sync the entire folder.
      #   The ordering here matters because syncing dag folder first will wipe the existing remark, data, and xls folders before adding again later.
      - run:
          command:    |
              set -e
              echo "Syncing plugins folder..."
              if [ "$(ls -A ./remark_airflow/plugins)" ]; then
                gsutil rsync -d -r ./remark_airflow/plugins gs://"$BUCKET"/plugins/
              else
                echo "EMPTY PLUGINS FOLDER"
              fi
      - run:
          command:    |
              set -e
              echo "Syncing dags folder..."
              if [ "$(ls -A ./remark_airflow/dags)" ]; then
                gsutil rsync -d -r ./remark_airflow/dags gs://"$BUCKET"/dags/
              else
                echo "EMPTY DAGS FOLDER"
              fi
      - run:
          command:    |
            set -e
            echo "Syncing remark app into environment bucket..."
            gsutil rsync -d -r ./remark gs://"$BUCKET"/dags/remark
      - run:
          command:    |
            set -e
            echo "Syncing data folder into environment bucket..."
            gsutil rsync -d -r ./remark gs://"$BUCKET"/dags/data
      - run:
          command:    |
            set -e
            echo "Syncing xls folder into environment bucket..."
            gsutil rsync -d -r ./remark gs://"$BUCKET"/dags/xls


  deploy-staging:
    docker:
      - image: circleci/python:3.7.2-node

    working_directory: ~/repo

    steps:
      - run:
          name: Install Heroku Tools
          command: curl https://cli-assets.heroku.com/install.sh | sh
      - checkout
      - aws-cli/install
      - run:
          name: Install Redis CLI
          command: |
            cd /tmp
            wget http://download.redis.io/redis-stable.tar.gz
            tar xvzf redis-stable.tar.gz
            cd redis-stable
            make
            sudo cp src/redis-cli /usr/local/bin/
            sudo chmod 755 /usr/local/bin/redis-cli
      - run:
          name: Install Frontend deps
          command: yarn install
      - run:
          name: Build Frontend
          command: yarn run build:prod
      - run:
          name: Maintenance Mode On
          command: heroku maintenance:on --app remarkably-staging
      - run:
          name: Deploy Staging to Heroku
          command: |
            git push https://heroku:$HEROKU_API_KEY@git.heroku.com/remarkably-staging.git staging:master --force
      - run:
          name: Reset Database
          command: heroku pg:reset --app remarkably-staging DATABASE_URL --confirm remarkably-staging
      - run:
          name: Migrate Database Schema
          command: heroku run -x -a remarkably-staging python manage.py migrate
      - run:
          name: Load fixture data
          command: heroku run -x -a remarkably-staging python manage.py loaddata data/dumped/latest.json
      - run:
          name: Delete Redis Cache
          command: redis-cli -u `heroku redis:credentials -a remarkably-staging REDIS_URL` flushall
      - run:
          name: Deploy artifact to S3
          command: yarn run deploy-frontend-staging
      - run:
          name: Invalidate CDN for staging Frontend
          command: yarn run invalidate-cdn-staging
      - run:
          name: Maintenance Mode Off
          command: heroku maintenance:off --app remarkably-staging

  deploy-production:
    docker:
      - image: circleci/python:3.7.2-node

    working_directory: ~/repo

    steps:
      - run:
          name: Install Heroku Tools
          command: curl https://cli-assets.heroku.com/install.sh | sh
      - checkout
      - aws-cli/install
      - run:
          name: Install Redis CLI
          command: |
            cd /tmp
            wget http://download.redis.io/redis-stable.tar.gz
            tar xvzf redis-stable.tar.gz
            cd redis-stable
            make
            sudo cp src/redis-cli /usr/local/bin/
            sudo chmod 755 /usr/local/bin/redis-cli
      - run:
          name: Install Frontend deps
          command: yarn install
      - run:
          name: Build Frontend
          command: yarn run build:prod
      - run:
          name: Maintenance Mode On
          command: heroku maintenance:on --app remarkably
      - run:
          name: Backup Production Postgres
          command: heroku pg:backups:capture --app remarkably
      - run:
          name: Deploy Master to Heroku Production
          command: |
            git push https://heroku:$HEROKU_API_KEY@git.heroku.com/remarkably.git master:master --force
      - run:
          name: Migrate Database Schema
          command: heroku run -x -a remarkably python manage.py migrate
      - run:
          name: Delete Redis Cache
          command: redis-cli -u `heroku redis:credentials -a remarkably REDIS_URL` flushall
      - run:
          name: Deploy artifact to S3
          command: yarn run deploy-frontend-production
      - run:
          name: Invalidate CDN for staging Frontend
          command: yarn run invalidate-cdn-production
      - run:
          name: Maintenance Mode Off
          command: heroku maintenance:off --app remarkably

  deploy-storybook:
    docker:
      - image: circleci/python:3.7.2-node

    working_directory: ~/repo

    steps:
      - checkout

      # Download and cache dependencies
      - restore_cache:
          keys:
            - v1-dependencies-{{ checksum "package.json" }}
            # fallback to using the latest cache if no exact match is found
            - v1-dependencies-

      - run: yarn install --silent
      - save_cache:
          paths:
            - node_modules
          key: v1-dependencies-{{ checksum "package.json" }}
      # build & deploy the storybook!
      - run: yarn deploy-storybook

workflows:
  version: 2.1

  tests-only:
    jobs:
      - deploy-airflow-staging:
          filters:
            branches:
              ignore:
                - staging
                - master
#      - build:
#          filters:
#            branches:
#              ignore:
#                - staging
#                - master

#  build-deploy:
#    jobs:
#      - build:
#          context: stage-aws
#          filters:
#            branches:
#              only: staging
#      - deploy-airflow-staging:
#          context: stage-aws
#          requires:
#            - build
#          filters:
#            branches:
#              only: staging
#      - deploy-staging:
#          context: stage-aws
#          requires:
#            - deploy-airflow-staging
#          filters:
#            branches:
#              only: staging
#      - deploy-storybook:
#          context: stage-aws
#          requires:
#            - deploy-staging
#          filters:
#            branches:
#              only: staging
#
#  build-production:
#    jobs:
#      - build:
#          context: deploy-production
#          filters:
#            branches:
#              only: master
#      - hold:
#          type: approval
#          requires:
#            - build
#      - deploy-production:
#          context: deploy-production
#          requires:
#            - hold
